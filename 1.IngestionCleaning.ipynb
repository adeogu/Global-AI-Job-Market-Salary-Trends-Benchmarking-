{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2d6d3499",
   "metadata": {},
   "source": [
    "# Ingestion & Cleaning — USA Salaries"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8ba2559-58b5-4302-87bd-893178f23550",
   "metadata": {},
   "source": [
    "**Setup and Ingestion**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9a4d391c",
   "metadata": {},
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "DLL load failed while importing lib: The specified procedure could not be found.",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mImportError\u001b[39m                               Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[1]\u001b[39m\u001b[32m, line 6\u001b[39m\n\u001b[32m      4\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mmatplotlib\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mpyplot\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mplt\u001b[39;00m\n\u001b[32m      5\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mseaborn\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01msns\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m6\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01msklearn\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mensemble\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m RandomForestRegressor\n\u001b[32m      7\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01msklearn\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mmodel_selection\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m train_test_split\n\u001b[32m      8\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01msklearn\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mpreprocessing\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m OneHotEncoder\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\anaconda3\\envs\\dmml\\Lib\\site-packages\\sklearn\\__init__.py:73\u001b[39m\n\u001b[32m     62\u001b[39m \u001b[38;5;66;03m# `_distributor_init` allows distributors to run custom init code.\u001b[39;00m\n\u001b[32m     63\u001b[39m \u001b[38;5;66;03m# For instance, for the Windows wheel, this is used to pre-load the\u001b[39;00m\n\u001b[32m     64\u001b[39m \u001b[38;5;66;03m# vcomp shared library runtime for OpenMP embedded in the sklearn/.libs\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m     67\u001b[39m \u001b[38;5;66;03m# later is linked to the OpenMP runtime to make it possible to introspect\u001b[39;00m\n\u001b[32m     68\u001b[39m \u001b[38;5;66;03m# it and importing it first would fail if the OpenMP dll cannot be found.\u001b[39;00m\n\u001b[32m     69\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01m.\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m (  \u001b[38;5;66;03m# noqa: F401 E402\u001b[39;00m\n\u001b[32m     70\u001b[39m     __check_build,\n\u001b[32m     71\u001b[39m     _distributor_init,\n\u001b[32m     72\u001b[39m )\n\u001b[32m---> \u001b[39m\u001b[32m73\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01m.\u001b[39;00m\u001b[34;01mbase\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m clone  \u001b[38;5;66;03m# noqa: E402\u001b[39;00m\n\u001b[32m     74\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01m.\u001b[39;00m\u001b[34;01mutils\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01m_show_versions\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m show_versions  \u001b[38;5;66;03m# noqa: E402\u001b[39;00m\n\u001b[32m     76\u001b[39m _submodules = [\n\u001b[32m     77\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33mcalibration\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m     78\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33mcluster\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m   (...)\u001b[39m\u001b[32m    114\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33mcompose\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m    115\u001b[39m ]\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\anaconda3\\envs\\dmml\\Lib\\site-packages\\sklearn\\base.py:19\u001b[39m\n\u001b[32m     17\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01m.\u001b[39;00m\u001b[34;01m_config\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m config_context, get_config\n\u001b[32m     18\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01m.\u001b[39;00m\u001b[34;01mexceptions\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m InconsistentVersionWarning\n\u001b[32m---> \u001b[39m\u001b[32m19\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01m.\u001b[39;00m\u001b[34;01mutils\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01m_metadata_requests\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m _MetadataRequester, _routing_enabled\n\u001b[32m     20\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01m.\u001b[39;00m\u001b[34;01mutils\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01m_missing\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m is_scalar_nan\n\u001b[32m     21\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01m.\u001b[39;00m\u001b[34;01mutils\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01m_param_validation\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m validate_parameter_constraints\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\anaconda3\\envs\\dmml\\Lib\\site-packages\\sklearn\\utils\\__init__.py:9\u001b[39m\n\u001b[32m      7\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01m.\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m metadata_routing\n\u001b[32m      8\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01m.\u001b[39;00m\u001b[34;01m_bunch\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m Bunch\n\u001b[32m----> \u001b[39m\u001b[32m9\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01m.\u001b[39;00m\u001b[34;01m_chunking\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m gen_batches, gen_even_slices\n\u001b[32m     11\u001b[39m \u001b[38;5;66;03m# Make _safe_indexing importable from here for backward compat as this particular\u001b[39;00m\n\u001b[32m     12\u001b[39m \u001b[38;5;66;03m# helper is considered semi-private and typically very useful for third-party\u001b[39;00m\n\u001b[32m     13\u001b[39m \u001b[38;5;66;03m# libraries that want to comply with scikit-learn's estimator API. In particular,\u001b[39;00m\n\u001b[32m     14\u001b[39m \u001b[38;5;66;03m# _safe_indexing was included in our public API documentation despite the leading\u001b[39;00m\n\u001b[32m     15\u001b[39m \u001b[38;5;66;03m# `_` in its name.\u001b[39;00m\n\u001b[32m     16\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01m.\u001b[39;00m\u001b[34;01m_indexing\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[32m     17\u001b[39m     _safe_indexing,  \u001b[38;5;66;03m# noqa: F401\u001b[39;00m\n\u001b[32m     18\u001b[39m     resample,\n\u001b[32m     19\u001b[39m     shuffle,\n\u001b[32m     20\u001b[39m )\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\anaconda3\\envs\\dmml\\Lib\\site-packages\\sklearn\\utils\\_chunking.py:11\u001b[39m\n\u001b[32m      8\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mnumpy\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mnp\u001b[39;00m\n\u001b[32m     10\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01m.\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01m_config\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m get_config\n\u001b[32m---> \u001b[39m\u001b[32m11\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01m.\u001b[39;00m\u001b[34;01m_param_validation\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m Interval, validate_params\n\u001b[32m     14\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mchunk_generator\u001b[39m(gen, chunksize):\n\u001b[32m     15\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"Chunk generator, ``gen`` into lists of length ``chunksize``. The last\u001b[39;00m\n\u001b[32m     16\u001b[39m \u001b[33;03m    chunk may have a length less than ``chunksize``.\"\"\"\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\anaconda3\\envs\\dmml\\Lib\\site-packages\\sklearn\\utils\\_param_validation.py:17\u001b[39m\n\u001b[32m     14\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mscipy\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01msparse\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m csr_matrix, issparse\n\u001b[32m     16\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01m.\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01m_config\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m config_context, get_config\n\u001b[32m---> \u001b[39m\u001b[32m17\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01m.\u001b[39;00m\u001b[34;01mvalidation\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m _is_arraylike_not_scalar\n\u001b[32m     20\u001b[39m \u001b[38;5;28;01mclass\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mInvalidParameterError\u001b[39;00m(\u001b[38;5;167;01mValueError\u001b[39;00m, \u001b[38;5;167;01mTypeError\u001b[39;00m):\n\u001b[32m     21\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"Custom exception to be raised when the parameter of a class/method/function\u001b[39;00m\n\u001b[32m     22\u001b[39m \u001b[33;03m    does not have a valid type or value.\u001b[39;00m\n\u001b[32m     23\u001b[39m \u001b[33;03m    \"\"\"\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\anaconda3\\envs\\dmml\\Lib\\site-packages\\sklearn\\utils\\validation.py:21\u001b[39m\n\u001b[32m     19\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01m.\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m get_config \u001b[38;5;28;01mas\u001b[39;00m _get_config\n\u001b[32m     20\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01m.\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mexceptions\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m DataConversionWarning, NotFittedError, PositiveSpectrumWarning\n\u001b[32m---> \u001b[39m\u001b[32m21\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01m.\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mutils\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01m_array_api\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m _asarray_with_order, _is_numpy_namespace, get_namespace\n\u001b[32m     22\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01m.\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mutils\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mdeprecation\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m _deprecate_force_all_finite\n\u001b[32m     23\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01m.\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mutils\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mfixes\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m ComplexWarning, _preserve_dia_indices_dtype\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\anaconda3\\envs\\dmml\\Lib\\site-packages\\sklearn\\utils\\_array_api.py:20\u001b[39m\n\u001b[32m     18\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01m.\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mexternals\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m array_api_extra \u001b[38;5;28;01mas\u001b[39;00m xpx\n\u001b[32m     19\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01m.\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mexternals\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01marray_api_compat\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m numpy \u001b[38;5;28;01mas\u001b[39;00m np_compat\n\u001b[32m---> \u001b[39m\u001b[32m20\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01m.\u001b[39;00m\u001b[34;01mfixes\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m parse_version\n\u001b[32m     22\u001b[39m \u001b[38;5;66;03m# TODO: complete __all__\u001b[39;00m\n\u001b[32m     23\u001b[39m __all__ = [\u001b[33m\"\u001b[39m\u001b[33mxpx\u001b[39m\u001b[33m\"\u001b[39m]  \u001b[38;5;66;03m# we import xpx here just to re-export it, need this to appease ruff\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\anaconda3\\envs\\dmml\\Lib\\site-packages\\sklearn\\utils\\fixes.py:421\u001b[39m\n\u001b[32m    419\u001b[39m PYARROW_VERSION_BELOW_17 = \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[32m    420\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m421\u001b[39m     \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mpyarrow\u001b[39;00m\n\u001b[32m    423\u001b[39m     pyarrow_version = parse_version(pyarrow.__version__)\n\u001b[32m    424\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m pyarrow_version < parse_version(\u001b[33m\"\u001b[39m\u001b[33m17.0.0\u001b[39m\u001b[33m\"\u001b[39m):\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\anaconda3\\envs\\dmml\\Lib\\site-packages\\pyarrow\\__init__.py:61\u001b[39m\n\u001b[32m     58\u001b[39m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mImportError\u001b[39;00m:\n\u001b[32m     59\u001b[39m         __version__ = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m61\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mpyarrow\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mlib\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01m_lib\u001b[39;00m\n\u001b[32m     62\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mpyarrow\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mlib\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m (BuildInfo, RuntimeInfo, set_timezone_db_path,\n\u001b[32m     63\u001b[39m                          MonthDayNano, VersionInfo, cpp_build_info,\n\u001b[32m     64\u001b[39m                          cpp_version, cpp_version_info, runtime_info,\n\u001b[32m     65\u001b[39m                          cpu_count, set_cpu_count, enable_signal_handlers,\n\u001b[32m     66\u001b[39m                          io_thread_count, set_io_thread_count)\n\u001b[32m     69\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mshow_versions\u001b[39m():\n",
      "\u001b[31mImportError\u001b[39m: DLL load failed while importing lib: The specified procedure could not be found."
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "\n",
    "\n",
    "pd.set_option('display.max_columns', 200)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "494e865f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<bound method NDFrame.head of         job_id                    job_title  salary_usd salary_currency  \\\n",
       "0      AI00001        AI Research Scientist       90376             USD   \n",
       "1      AI00002         AI Software Engineer       61895             USD   \n",
       "2      AI00003                AI Specialist      152626             USD   \n",
       "3      AI00004                 NLP Engineer       80215             USD   \n",
       "4      AI00005                AI Consultant       54624             EUR   \n",
       "...        ...                          ...         ...             ...   \n",
       "14995  AI14996            Robotics Engineer       38604             USD   \n",
       "14996  AI14997  Machine Learning Researcher       57811             GBP   \n",
       "14997  AI14998                 NLP Engineer      189490             USD   \n",
       "14998  AI14999                   Head of AI       79461             EUR   \n",
       "14999  AI15000     Computer Vision Engineer       56481             USD   \n",
       "\n",
       "      experience_level employment_type company_location company_size  \\\n",
       "0                   SE              CT            China            M   \n",
       "1                   EN              CT           Canada            M   \n",
       "2                   MI              FL      Switzerland            L   \n",
       "3                   SE              FL            India            M   \n",
       "4                   EN              PT           France            S   \n",
       "...                ...             ...              ...          ...   \n",
       "14995               EN              FL          Finland            S   \n",
       "14996               EN              CT   United Kingdom            M   \n",
       "14997               EX              CT      South Korea            L   \n",
       "14998               EN              FT      Netherlands            M   \n",
       "14999               MI              PT          Austria            S   \n",
       "\n",
       "      employee_residence  remote_ratio  \\\n",
       "0                  China            50   \n",
       "1                Ireland           100   \n",
       "2            South Korea             0   \n",
       "3                  India            50   \n",
       "4              Singapore           100   \n",
       "...                  ...           ...   \n",
       "14995            Finland            50   \n",
       "14996     United Kingdom             0   \n",
       "14997        South Korea            50   \n",
       "14998        Netherlands             0   \n",
       "14999            Austria            50   \n",
       "\n",
       "                                       required_skills education_required  \\\n",
       "0             Tableau, PyTorch, Kubernetes, Linux, NLP           Bachelor   \n",
       "1      Deep Learning, AWS, Mathematics, Python, Docker             Master   \n",
       "2         Kubernetes, Deep Learning, Java, Hadoop, NLP          Associate   \n",
       "3                            Scala, SQL, Linux, Python                PhD   \n",
       "4                         MLOps, Java, Tableau, Python             Master   \n",
       "...                                                ...                ...   \n",
       "14995                          Java, Kubernetes, Azure           Bachelor   \n",
       "14996          Mathematics, Docker, SQL, Deep Learning             Master   \n",
       "14997                                Scala, Spark, NLP          Associate   \n",
       "14998        Java, Computer Vision, Python, TensorFlow                PhD   \n",
       "14999    Scala, Azure, Deep Learning, GCP, Mathematics                PhD   \n",
       "\n",
       "       years_experience       industry posting_date application_deadline  \\\n",
       "0                     9     Automotive   2024-10-18           2024-11-07   \n",
       "1                     1          Media   2024-11-20           2025-01-11   \n",
       "2                     2      Education   2025-03-18           2025-04-07   \n",
       "3                     7     Consulting   2024-12-23           2025-02-24   \n",
       "4                     0          Media   2025-04-15           2025-06-23   \n",
       "...                 ...            ...          ...                  ...   \n",
       "14995                 1         Energy   2025-02-06           2025-03-25   \n",
       "14996                 0     Government   2024-10-16           2024-10-30   \n",
       "14997                17  Manufacturing   2024-03-19           2024-05-02   \n",
       "14998                 1    Real Estate   2024-03-22           2024-04-23   \n",
       "14999                 2     Technology   2024-07-18           2024-08-10   \n",
       "\n",
       "       job_description_length  benefits_score       company_name  \n",
       "0                        1076             5.9    Smart Analytics  \n",
       "1                        1268             5.2       TechCorp Inc  \n",
       "2                        1974             9.4    Autonomous Tech  \n",
       "3                        1345             8.6     Future Systems  \n",
       "4                        1989             6.6  Advanced Robotics  \n",
       "...                       ...             ...                ...  \n",
       "14995                    1635             7.9  Advanced Robotics  \n",
       "14996                    1624             8.2    Smart Analytics  \n",
       "14997                    1336             7.4     AI Innovations  \n",
       "14998                    1935             5.6    Smart Analytics  \n",
       "14999                    2492             7.6     AI Innovations  \n",
       "\n",
       "[15000 rows x 19 columns]>"
      ]
     },
     "execution_count": 103,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "df = pd.read_csv('data/ai_job_dataset0.csv')\n",
    "df.head"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37fc815f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Initial DataFrame Info:\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 15000 entries, 0 to 14999\n",
      "Data columns (total 19 columns):\n",
      " #   Column                  Non-Null Count  Dtype  \n",
      "---  ------                  --------------  -----  \n",
      " 0   job_id                  15000 non-null  object \n",
      " 1   job_title               15000 non-null  object \n",
      " 2   salary_usd              15000 non-null  int64  \n",
      " 3   salary_currency         15000 non-null  object \n",
      " 4   experience_level        15000 non-null  object \n",
      " 5   employment_type         15000 non-null  object \n",
      " 6   company_location        15000 non-null  object \n",
      " 7   company_size            15000 non-null  object \n",
      " 8   employee_residence      15000 non-null  object \n",
      " 9   remote_ratio            15000 non-null  int64  \n",
      " 10  required_skills         15000 non-null  object \n",
      " 11  education_required      15000 non-null  object \n",
      " 12  years_experience        15000 non-null  int64  \n",
      " 13  industry                15000 non-null  object \n",
      " 14  posting_date            15000 non-null  object \n",
      " 15  application_deadline    15000 non-null  object \n",
      " 16  job_description_length  15000 non-null  int64  \n",
      " 17  benefits_score          15000 non-null  float64\n",
      " 18  company_name            15000 non-null  object \n",
      "dtypes: float64(1), int64(4), object(14)\n",
      "memory usage: 2.2+ MB\n"
     ]
    }
   ],
   "source": [
    "print(\"\\nInitial DataFrame Info:\")\n",
    "df.info()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ccdb5dc",
   "metadata": {},
   "source": [
    "### Quick schema check — align to proposal columns\n",
    "We expect the dataset to contain (per proposal):\n",
    "- job_id, job_title, salary_usd, salary_currency, salary_local, experience_level,\n",
    "- employment_type, job_category, company_location, company_size, employee_residence,\n",
    "- remote_ratio, required_skills, education_required, years_experience, industry,\n",
    "- posting_date, application_deadline, job_description_length, benefits_score\n",
    "\n",
    "We'll display the actual columns present and map / rename where needed."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f1e23fd-5ad9-4e87-9386-e391eaa3b8d2",
   "metadata": {},
   "source": [
    "**Column Normalization**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08ecb181",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Canonical columns:\n",
      " ['job_id', 'job_title', 'salary_usd', 'salary_currency', 'experience_level', 'employment_type', 'company_location', 'company_size', 'employee_residence', 'remote_ratio', 'required_skills', 'education_required', 'years_experience', 'industry', 'posting_date', 'application_deadline', 'job_description_length', 'benefits_score', 'company_name']\n"
     ]
    }
   ],
   "source": [
    "def canonicalize_columns(df_in):\n",
    "    \"\"\"Lowercases and standardizes column names.\"\"\"\n",
    "    df_out = df_in.copy()\n",
    "    col_map = {c: c.strip().lower().replace('-', '_').replace(' ', '_') for c in df_out.columns}\n",
    "    df_out = df_out.rename(columns=col_map)\n",
    "    return df_out\n",
    "\n",
    "df_cleaned = canonicalize_columns(df)\n",
    "print('Canonical columns:\\n', df_cleaned.columns.tolist())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "542fef7a-1746-4cdb-aaa7-12afb21ff74b",
   "metadata": {},
   "source": [
    "**Dropping Unnecessary Columns**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ccbec018",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dropped columns: ['salary_currency']\n",
      "DataFrame shape after drops: (15000, 18)\n"
     ]
    }
   ],
   "source": [
    "cols_to_drop = ['salary_local', 'salary_currency']\n",
    "\n",
    "existing_cols_to_drop = [col for col in cols_to_drop if col in df_cleaned.columns]\n",
    "df_cleaned.drop(columns=existing_cols_to_drop, inplace=True)\n",
    "\n",
    "print(f\"Dropped columns: {existing_cols_to_drop}\")\n",
    "print(f\"DataFrame shape after drops: {df_cleaned.shape}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3bdb706",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No duplicates found for job_id.\n"
     ]
    }
   ],
   "source": [
    "duplicate_count = df_cleaned.duplicated(subset=['job_id']).sum()\n",
    "\n",
    "if duplicate_count > 0:\n",
    "    print(f\"Found {duplicate_count} duplicate job_id entries.\")\n",
    "else:\n",
    "    print(\"No duplicates found for job_id.\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae518c9d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Rows with missing salary_usd: 0\n"
     ]
    }
   ],
   "source": [
    "missing_salary_count = df_cleaned['salary_usd'].isna().sum()\n",
    "print(f\"Rows with missing salary_usd: {missing_salary_count}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3188acb-734a-4758-9333-6f8de4621474",
   "metadata": {},
   "source": [
    "**Data Type Conversions**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c983466",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "posting_date: 0 invalid date entries\n",
      "application_deadline: 0 invalid date entries\n",
      "years_experience: 0 invalid numeric entries\n",
      "job_description_length: 0 invalid numeric entries\n",
      "benefits_score: 0 invalid numeric entries\n"
     ]
    }
   ],
   "source": [
    "\n",
    "for dcol in ['posting_date', 'application_deadline']:\n",
    "    if dcol in df_cleaned.columns:\n",
    "        temp = pd.to_datetime(df_cleaned[dcol], errors='coerce')\n",
    "        invalid_dates = temp.isna().sum()\n",
    "        print(f\"{dcol}: {invalid_dates} invalid date entries\")\n",
    "\n",
    "for ncol in ['years_experience', 'job_description_length', 'benefits_score']:\n",
    "    if ncol in df_cleaned.columns:\n",
    "        temp = pd.to_numeric(df_cleaned[ncol], errors='coerce')\n",
    "        invalid_numeric = temp.isna().sum()\n",
    "        print(f\"{ncol}: {invalid_numeric} invalid numeric entries\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c373a5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "if 'experience_level' in df_cleaned.columns:\n",
    "    exp_map = {'EN': 'Entry-Level', 'MI': 'Mid-Level', 'SE': 'Senior-Level', 'EX': 'Executive'}\n",
    "    df_cleaned['experience_level'] = df_cleaned['experience_level'].map(exp_map).fillna(df_cleaned['experience_level'])\n",
    "\n",
    "if 'company_size' in df_cleaned.columns:\n",
    "    size_map = {'S': 'Small (<50)', 'M': 'Medium (50-250)', 'L': 'Large (>250)'}\n",
    "    df_cleaned['company_size'] = df_cleaned['company_size'].map(size_map).fillna(df_cleaned['company_size'])\n",
    "\n",
    "if 'employment_type' in df_cleaned.columns:\n",
    "    emp_map = {'FT': 'Full-Time', 'PT': 'Part-Time', 'CT': 'Contract', 'FL': 'Freelance'}\n",
    "    df_cleaned['employment_type'] = df_cleaned['employment_type'].map(emp_map).fillna(df_cleaned['employment_type'])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bdd70e6b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Standardized 'experience_level', 'company_size', 'employment_type', and created 'remote_type'.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>job_id</th>\n",
       "      <th>job_title</th>\n",
       "      <th>salary_usd</th>\n",
       "      <th>salary_currency</th>\n",
       "      <th>experience_level</th>\n",
       "      <th>employment_type</th>\n",
       "      <th>company_location</th>\n",
       "      <th>company_size</th>\n",
       "      <th>employee_residence</th>\n",
       "      <th>remote_ratio</th>\n",
       "      <th>required_skills</th>\n",
       "      <th>education_required</th>\n",
       "      <th>years_experience</th>\n",
       "      <th>industry</th>\n",
       "      <th>posting_date</th>\n",
       "      <th>application_deadline</th>\n",
       "      <th>job_description_length</th>\n",
       "      <th>benefits_score</th>\n",
       "      <th>company_name</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>AI00001</td>\n",
       "      <td>AI Research Scientist</td>\n",
       "      <td>90376</td>\n",
       "      <td>USD</td>\n",
       "      <td>SE</td>\n",
       "      <td>CT</td>\n",
       "      <td>China</td>\n",
       "      <td>M</td>\n",
       "      <td>China</td>\n",
       "      <td>50</td>\n",
       "      <td>Tableau, PyTorch, Kubernetes, Linux, NLP</td>\n",
       "      <td>Bachelor</td>\n",
       "      <td>9</td>\n",
       "      <td>Automotive</td>\n",
       "      <td>2024-10-18</td>\n",
       "      <td>2024-11-07</td>\n",
       "      <td>1076</td>\n",
       "      <td>5.9</td>\n",
       "      <td>Smart Analytics</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>AI00002</td>\n",
       "      <td>AI Software Engineer</td>\n",
       "      <td>61895</td>\n",
       "      <td>USD</td>\n",
       "      <td>EN</td>\n",
       "      <td>CT</td>\n",
       "      <td>Canada</td>\n",
       "      <td>M</td>\n",
       "      <td>Ireland</td>\n",
       "      <td>100</td>\n",
       "      <td>Deep Learning, AWS, Mathematics, Python, Docker</td>\n",
       "      <td>Master</td>\n",
       "      <td>1</td>\n",
       "      <td>Media</td>\n",
       "      <td>2024-11-20</td>\n",
       "      <td>2025-01-11</td>\n",
       "      <td>1268</td>\n",
       "      <td>5.2</td>\n",
       "      <td>TechCorp Inc</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>AI00003</td>\n",
       "      <td>AI Specialist</td>\n",
       "      <td>152626</td>\n",
       "      <td>USD</td>\n",
       "      <td>MI</td>\n",
       "      <td>FL</td>\n",
       "      <td>Switzerland</td>\n",
       "      <td>L</td>\n",
       "      <td>South Korea</td>\n",
       "      <td>0</td>\n",
       "      <td>Kubernetes, Deep Learning, Java, Hadoop, NLP</td>\n",
       "      <td>Associate</td>\n",
       "      <td>2</td>\n",
       "      <td>Education</td>\n",
       "      <td>2025-03-18</td>\n",
       "      <td>2025-04-07</td>\n",
       "      <td>1974</td>\n",
       "      <td>9.4</td>\n",
       "      <td>Autonomous Tech</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>AI00004</td>\n",
       "      <td>NLP Engineer</td>\n",
       "      <td>80215</td>\n",
       "      <td>USD</td>\n",
       "      <td>SE</td>\n",
       "      <td>FL</td>\n",
       "      <td>India</td>\n",
       "      <td>M</td>\n",
       "      <td>India</td>\n",
       "      <td>50</td>\n",
       "      <td>Scala, SQL, Linux, Python</td>\n",
       "      <td>PhD</td>\n",
       "      <td>7</td>\n",
       "      <td>Consulting</td>\n",
       "      <td>2024-12-23</td>\n",
       "      <td>2025-02-24</td>\n",
       "      <td>1345</td>\n",
       "      <td>8.6</td>\n",
       "      <td>Future Systems</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>AI00005</td>\n",
       "      <td>AI Consultant</td>\n",
       "      <td>54624</td>\n",
       "      <td>EUR</td>\n",
       "      <td>EN</td>\n",
       "      <td>PT</td>\n",
       "      <td>France</td>\n",
       "      <td>S</td>\n",
       "      <td>Singapore</td>\n",
       "      <td>100</td>\n",
       "      <td>MLOps, Java, Tableau, Python</td>\n",
       "      <td>Master</td>\n",
       "      <td>0</td>\n",
       "      <td>Media</td>\n",
       "      <td>2025-04-15</td>\n",
       "      <td>2025-06-23</td>\n",
       "      <td>1989</td>\n",
       "      <td>6.6</td>\n",
       "      <td>Advanced Robotics</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    job_id              job_title  salary_usd salary_currency  \\\n",
       "0  AI00001  AI Research Scientist       90376             USD   \n",
       "1  AI00002   AI Software Engineer       61895             USD   \n",
       "2  AI00003          AI Specialist      152626             USD   \n",
       "3  AI00004           NLP Engineer       80215             USD   \n",
       "4  AI00005          AI Consultant       54624             EUR   \n",
       "\n",
       "  experience_level employment_type company_location company_size  \\\n",
       "0               SE              CT            China            M   \n",
       "1               EN              CT           Canada            M   \n",
       "2               MI              FL      Switzerland            L   \n",
       "3               SE              FL            India            M   \n",
       "4               EN              PT           France            S   \n",
       "\n",
       "  employee_residence  remote_ratio  \\\n",
       "0              China            50   \n",
       "1            Ireland           100   \n",
       "2        South Korea             0   \n",
       "3              India            50   \n",
       "4          Singapore           100   \n",
       "\n",
       "                                   required_skills education_required  \\\n",
       "0         Tableau, PyTorch, Kubernetes, Linux, NLP           Bachelor   \n",
       "1  Deep Learning, AWS, Mathematics, Python, Docker             Master   \n",
       "2     Kubernetes, Deep Learning, Java, Hadoop, NLP          Associate   \n",
       "3                        Scala, SQL, Linux, Python                PhD   \n",
       "4                     MLOps, Java, Tableau, Python             Master   \n",
       "\n",
       "   years_experience    industry posting_date application_deadline  \\\n",
       "0                 9  Automotive   2024-10-18           2024-11-07   \n",
       "1                 1       Media   2024-11-20           2025-01-11   \n",
       "2                 2   Education   2025-03-18           2025-04-07   \n",
       "3                 7  Consulting   2024-12-23           2025-02-24   \n",
       "4                 0       Media   2025-04-15           2025-06-23   \n",
       "\n",
       "   job_description_length  benefits_score       company_name  \n",
       "0                    1076             5.9    Smart Analytics  \n",
       "1                    1268             5.2       TechCorp Inc  \n",
       "2                    1974             9.4    Autonomous Tech  \n",
       "3                    1345             8.6     Future Systems  \n",
       "4                    1989             6.6  Advanced Robotics  "
      ]
     },
     "execution_count": 114,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "def remote_type_from_ratio(x):\n",
    "    if pd.isna(x): return 'Unknown'\n",
    "    if x == 0: return 'Onsite'\n",
    "    if x == 100: return 'Fully Remote'\n",
    "    if 0 < x < 100: return 'Hybrid'\n",
    "    return 'Unknown'\n",
    "\n",
    "if 'remote_ratio' in df_cleaned.columns:\n",
    "    df_cleaned['remote_type'] = df_cleaned['remote_ratio'].apply(remote_type_from_ratio)\n",
    "\n",
    "print(\"Standardized 'experience_level', 'company_size', 'employment_type', and created 'remote_type'.\")\n",
    "\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d07431e0-3a34-43f6-89e7-54c7a07872d0",
   "metadata": {},
   "source": [
    "**Feature Engineering: Skills Count**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b44712c9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Created 'skills_list' and 'skills_count' features.\n",
      "                                   required_skills  \\\n",
      "0         Tableau, PyTorch, Kubernetes, Linux, NLP   \n",
      "1  Deep Learning, AWS, Mathematics, Python, Docker   \n",
      "2     Kubernetes, Deep Learning, Java, Hadoop, NLP   \n",
      "3                        Scala, SQL, Linux, Python   \n",
      "4                     MLOps, Java, Tableau, Python   \n",
      "\n",
      "                                         skills_list  skills_count  \n",
      "0         [tableau, pytorch, kubernetes, linux, nlp]             5  \n",
      "1  [deep learning, aws, mathematics, python, docker]             5  \n",
      "2     [kubernetes, deep learning, java, hadoop, nlp]             5  \n",
      "3                        [scala, sql, linux, python]             4  \n",
      "4                     [mlops, java, tableau, python]             4  \n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "\n",
    "def parse_skills(text):\n",
    "    if pd.isna(text):\n",
    "        return []\n",
    "    parts = re.split(r'[;,|\\n]+', str(text))\n",
    "    skills = [p.strip().lower() for p in parts if p.strip()]\n",
    "    return list(dict.fromkeys(skills)) \n",
    "\n",
    "if 'required_skills' in df_cleaned.columns:\n",
    "    df_cleaned['skills_list'] = df_cleaned['required_skills'].apply(parse_skills)\n",
    "    df_cleaned['skills_count'] = df_cleaned['skills_list'].apply(len)\n",
    "    print(\"Created 'skills_list' and 'skills_count' features.\")\n",
    "else:\n",
    "    print(\"Warning: 'required_skills' column not found. Skipping skill processing.\")\n",
    "\n",
    "print(df_cleaned[['required_skills', 'skills_list', 'skills_count']].head())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ffa6126-08de-4303-a65d-babac7567f4c",
   "metadata": {},
   "source": [
    "**Numerical Imputation & Outlier Handling**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8366fb4",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['posting_date'] = pd.to_datetime(df['posting_date'])\n",
    "df['application_deadline'] = pd.to_datetime(df['application_deadline'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c252f2e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0        20\n",
       "1        52\n",
       "2        20\n",
       "3        63\n",
       "4        69\n",
       "         ..\n",
       "14995    47\n",
       "14996    14\n",
       "14997    44\n",
       "14998    32\n",
       "14999    23\n",
       "Name: application_duration, Length: 15000, dtype: int64"
      ]
     },
     "execution_count": 119,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['application_duration'] = (df['application_deadline'] - df['posting_date']).dt.days\n",
    "df['application_duration']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18488897",
   "metadata": {},
   "source": [
    "**Removing Outliers**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f710b996",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -----------------------------\n",
    "# Outlier Detection Using Prediction Intervals (GBR)\n",
    "# -----------------------------\n",
    "from sklearn.ensemble import GradientBoostingRegressor\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# -----------------------------\n",
    "# 1. Select numeric + categorical features\n",
    "# -----------------------------\n",
    "target = \"salary_usd\"\n",
    "\n",
    "# Drop rows with missing target\n",
    "df_model = df_cleaned.dropna(subset=[target]).copy()\n",
    "\n",
    "# Select features (customize as needed)\n",
    "feature_cols = [\n",
    "    \"experience_level\",\n",
    "    \"employment_type\",\n",
    "    \"company_location\",\n",
    "    \"company_size\",\n",
    "    \"employee_residence\",\n",
    "    \"remote_ratio\",\n",
    "    \"skills_count\",\n",
    "    \"years_experience\",\n",
    "    \"job_description_length\",\n",
    "    \"benefits_score\"\n",
    "]\n",
    "\n",
    "# Keep only existing columns\n",
    "feature_cols = [c for c in feature_cols if c in df_model.columns]\n",
    "\n",
    "# One-hot encode categoricals\n",
    "df_encoded = pd.get_dummies(df_model[feature_cols], drop_first=True)\n",
    "\n",
    "X = df_encoded.values\n",
    "y = df_model[target].values\n",
    "\n",
    "# -----------------------------\n",
    "# 2. Train quantile models\n",
    "# -----------------------------\n",
    "gbr_lower = GradientBoostingRegressor(loss=\"quantile\", alpha=0.05, n_estimators=300, max_depth=3)\n",
    "gbr_middle = GradientBoostingRegressor(loss=\"quantile\", alpha=0.50, n_estimators=300, max_depth=3)\n",
    "gbr_upper = GradientBoostingRegressor(loss=\"quantile\", alpha=0.95, n_estimators=300, max_depth=3)\n",
    "\n",
    "gbr_lower.fit(X, y)\n",
    "gbr_middle.fit(X, y)\n",
    "gbr_upper.fit(X, y)\n",
    "\n",
    "# -----------------------------\n",
    "# 3. Predict intervals\n",
    "# -----------------------------\n",
    "lower = gbr_lower.predict(X)\n",
    "pred   = gbr_middle.predict(X)\n",
    "upper = gbr_upper.predict(X)\n",
    "\n",
    "# -----------------------------\n",
    "# 4. Identify outliers\n",
    "# -----------------------------\n",
    "df_model[\"lower_bound\"] = lower\n",
    "df_model[\"pred_salary\"] = pred\n",
    "df_model[\"upper_bound\"] = upper\n",
    "\n",
    "df_model[\"is_outlier\"] = (df_model[target] < df_model[\"lower_bound\"]) | \\\n",
    "                         (df_model[target] > df_model[\"upper_bound\"])\n",
    "\n",
    "print(\"Outliers detected:\", df_model[\"is_outlier\"].sum())\n",
    "\n",
    "# -----------------------------\n",
    "# 5. Merge results back into df_cleaned\n",
    "# -----------------------------\n",
    "df_cleaned = df_cleaned.merge(\n",
    "    df_model[[\"job_id\", \"lower_bound\", \"pred_salary\", \"upper_bound\", \"is_outlier\"]],\n",
    "    on=\"job_id\",\n",
    "    how=\"left\"\n",
    ")\n",
    "\n",
    "print(\"Outlier detection complete.\")\n",
    "df_cleaned.head()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "049e7743-4162-4edb-a7f7-86bc4f657135",
   "metadata": {},
   "source": [
    "**Save Cleaned Data**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75201a41",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved cleaned global data to: data\\processed\\new_jobs_global.csv\n",
      "Final dataset shape: (15000, 21)\n"
     ]
    }
   ],
   "source": [
    "OUT_DIR = Path('data/processed')\n",
    "OUT_DIR.mkdir(parents=True, exist_ok=True)\n",
    "clean_path = OUT_DIR / 'new_jobs_global.csv'\n",
    "\n",
    "df_cleaned.to_csv(clean_path, index=False)\n",
    "\n",
    "print(f\"Saved cleaned global data to: {clean_path}\")\n",
    "print(f\"Final dataset shape: {df_cleaned.shape}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
